{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stock Market trading bot using Reinforcement Learning1.ipynb",
      "provenance": [],
      "mount_file_id": "1b3Pr4lmREPkTzV3XwrI9I-q9YNirf3eU",
      "authorship_tag": "ABX9TyNJe9PHZDIbR1itPdxRis66",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amenasetheru/Stock-Market-trading-bot-using-Reinforcement-Learning/blob/master/Stock_Market_trading_bot_using_Reinforcement_Learning1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqQ4l0SA9-DF",
        "colab_type": "text"
      },
      "source": [
        "# Stock Market trading bot using Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpSOKklO-QbS",
        "colab_type": "text"
      },
      "source": [
        "**Stage-1: Installing dependencies and environment setup**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EH_z4wH48647",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "00b634b4-aa9d-4243-de33-2fa7ce5c90df"
      },
      "source": [
        "!pip install pandas-datareader"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas-datareader in /usr/local/lib/python3.6/dist-packages (0.8.1)\n",
            "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from pandas-datareader) (2.23.0)\n",
            "Requirement already satisfied: pandas>=0.21 in /usr/local/lib/python3.6/dist-packages (from pandas-datareader) (1.0.5)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from pandas-datareader) (4.2.6)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->pandas-datareader) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->pandas-datareader) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->pandas-datareader) (2020.4.5.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->pandas-datareader) (2.9)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21->pandas-datareader) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21->pandas-datareader) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21->pandas-datareader) (1.18.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas>=0.21->pandas-datareader) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RyfQSDn_yCu",
        "colab_type": "text"
      },
      "source": [
        "**Stage-2: Importing project dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wjWi6MK_iXV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "2a8f2789-4f44-47af-ea82-d4cd35fac2e6"
      },
      "source": [
        "# Importing project dependencies\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas_datareader as data_reader\n",
        "\n",
        "from tqdm import tqdm_notebook, tqdm\n",
        "from collections import deque"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas_datareader/compat/__init__.py:7: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  from pandas.util.testing import assert_frame_equal\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JB8wP3nPAo2j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a9936aff-fd26-4c10-da4e-8f97f324133f"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05OQ-Wos961N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the algorithm\n",
        "class AI_Trader():\n",
        "\n",
        "\n",
        "  def __init__(self, state_size, action_space=3, model_name=\"AITrader\"):# Stay, Buy Sell,\n",
        "    \n",
        "    # Define all hyper parameters of the network\n",
        "    # The first two are just state size and action space defined as an object attribute\n",
        "\n",
        "    self.state_size = state_size\n",
        "    self.action_space = action_space\n",
        "\n",
        "\n",
        "    # This hyper parameter is the experience of repaly memory\n",
        "    # We store inside it 2000 elements \n",
        "    self.memory = deque(maxlen=2000)\n",
        "    # Initialize an empty list call inventory. This list will hold both\n",
        "    # Stocks since we cannot sell a stock if we haven't bought it before \n",
        "    self.inventory = []\n",
        "    self.model_name = model_name\n",
        "\n",
        "    # This gamma parameter helps us to miximize the current reward\n",
        "    # Over a long time reward. Set it between 0.9 and 1\n",
        "    self.gamma = 0.95\n",
        "    # The epsilon parameter is used to determine whether should we choose \n",
        "    # the random action or to use the model for it. Start by defining it to 1\n",
        "    # This means that at the very beginning of the training process when \n",
        "    # The network is not trained at all. All actions are performed randomly\n",
        "    # But over time, we want to decrease this number so we can stop using \n",
        "    # random actions and start using mostly our train network.\n",
        "    # Even though we have  a fully train network we still want the agent\n",
        "    # to take some random actions and that is for our environment exploation\n",
        "    # And that is where the next variable come into play. \n",
        "    self.epsilon = 1.0\n",
        "    # Here when the value of epsilon is equal to or less this number\n",
        "    # We will start decreasing it any further\n",
        "    self.epsilon_final = 0.01\n",
        "    # Define epsilon decay to less than 1. Here we set it to 0.995. It\n",
        "    # Helps to determine how fast to decrease epsilon\n",
        "    self.epsilon_decay = 0.995\n",
        "\n",
        "    # This function will create a network, initialize it and then \n",
        "    # Store in this cell argument for us\n",
        "    self.model = self.model_builder()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm1qPfWto4lE",
        "colab_type": "text"
      },
      "source": [
        "**Stage-3: Building the AI Trader Network** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyCIYPK7A0JF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build the model\n",
        "# This function doesn't take any argument. It jus provide a keyword self\n",
        "def model_builder(self):\n",
        "  model = tf.keras.models.Sequential()\n",
        "\n",
        "# Our states are nothing more than previous end days and stock prices\n",
        "# Over those days. \n",
        "# Our state is nothing more than a vector of numbers and we can simply\n",
        "# Use a fully connected network.\n",
        "  model.add(tf.keras.layers.Dense(units=32, activation=\"relu\",input_dim=self.state_size))\n",
        "  model.add(tf.keras.layers.Dense(units=64, activation=\"relu\"))\n",
        "  model.add(tf.keras.layers.Dense(units=128, activation=\"relu\"))\n",
        "\n",
        "  # For the dense layer, the number of units or neurons should be the same \n",
        "  # As the number of classes or in this case the number of actions.\n",
        "  # The activation function is linear because we are going to use a\n",
        "  # Mean squared error for our loss. We will see why in a few windows\n",
        "  # For we will modify our actions with our rewards which is a continuous\n",
        "  # Number and not a class \n",
        "  model.add(tf.keras.layers.Dense(units=self.action_space, activation=\"linear\"))\n",
        "\n",
        "  \n",
        "  model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.Adam(lr=0.001))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAlMV5fM5dY-",
        "colab_type": "text"
      },
      "source": [
        "### Building The Trade function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmNzIKRw2ozk",
        "colab_type": "text"
      },
      "source": [
        "We explain the role of the trade function:\n",
        "\n",
        "This function takes the state as an input and then generates a random number.\n",
        "\n",
        "If that generated number is less than or equal to epsilon(Notice that at the beginning, it is going to be always less than epsilon)\n",
        "\n",
        "This function will return totally a random generated number randing between 0 and 2. That is our actions\n",
        "\n",
        "If this is not the case, this function will call our model and perform prediction based on the inout sate and return only the action that has the highest probanility or the likehood between those free actions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJOtM3LtA0F9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build a trade function. It takes the state as an input and then\n",
        "# spills out an action to perform in a particular state\n",
        "# It takes only one argument and that is our state. For that state,\n",
        "# We need to determine whether should we use the random generated\n",
        "# Action or should we use our model to perform the action\n",
        "\n",
        "# To do that, we say if randdom.random is less than or equal to\n",
        "# Our epsilon, it is only in that case that we are going to return\n",
        "# A random action. So We will have to return random thus we will have\n",
        "# To call random.randrange function that can takes multiple argument\n",
        "# But in our case here, we are going to provide only stop point.\n",
        "# Which is self.action_space and ten it will randomly select \n",
        "# A number between 0 and 2. \n",
        "def trade(self, state):\n",
        "    if random.random()<= self.epsilon:\n",
        "        return random.randrange(self.action_space)\n",
        "  # if this is not the case or the random\n",
        "  # Generator number is bigger than epsilon then we are going to\n",
        "  # Use our model to use an action to perform. So to do that, we say\n",
        "  # Actions is equal to self.model.predict and for the argument here,\n",
        "  # We are going to provide our state and we need to return a\n",
        "  # Single number. So we are going to use np.argmax to return only one\n",
        "  # action which is the highest probability. As the argument for\n",
        "  # Argmax, we put actions of zero because of the output shape. \n",
        "    actions = self.model.predict(state)\n",
        "    return np.argmax(actions[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPCcnhZ45lB3",
        "colab_type": "text"
      },
      "source": [
        "### Buiding the Custom Training Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QltiHvVjA0DC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function will take a batch of saved data\n",
        "# And train the model based. It takes only one argument and that\n",
        "# Is the batch_sise which could be anywhere from 32 to 256 or even\n",
        "# More and this is an additional hyper parameter that we can play with\n",
        "# The first thing that we have to do is to select the data from the \n",
        "# experience replay momery.\n",
        "def batch_train(self, batch_size):\n",
        "  # So batch is equal to an empty list\n",
        "  batch = []\n",
        "  \n",
        "  # Then we have to iterate through the memory. For i in range then\n",
        "  # Len of self dot memory (and we need to remember that this memory\n",
        "  # Is defined up here as the deque data structure.) minus batch_size\n",
        "  # plus 1 because we are dealing here with a time constraint data.\n",
        "  # We don't want to randomly select samples from memory. We always\n",
        "  # Sample from the end of memory and this indexing method will help\n",
        "  # Us to get the exact number of points inside the batch of data.\n",
        "  # We complete our for loop by defining the end of the index to be the\n",
        "  # Len of self dot memory\n",
        "  for i in range(len(self.memory) - batch_size + 1,len(self.memory)):\n",
        "  # Now the for loop calls the batch list and append that element \n",
        "  # from the memory itself.\n",
        "    batch.append(self.memory[i])\n",
        "\n",
        "  \n",
        "  # Now we have the batch data it is time to iterate through it \n",
        "  # And to train the model for each sample.\n",
        "  # From that batch, let's write: for state, action, reward, next_state\n",
        "  # Done in batch with these four elements, we are iterating through \n",
        "  # That all information stored inside the batch of data.\n",
        "  # We need just te be careful here for the order of variables is\n",
        "  # Very important. \n",
        "  for state, action, reward, next_state, done in batch:\n",
        "  # Inside the for loop reward is equal to reward\n",
        "\n",
        "    reward = reward\n",
        "  \n",
        "  # Let's make sure that our agent is not in the terminal state\n",
        "  # So let's write if not done, we are doing this very simple check\n",
        "  # To make sure that our agent is not in the terminal state.\n",
        "  # We can calculate our reward in a different ways. If the agent is\n",
        "  # In a terminal state, we will use the current reward as a reward.\n",
        "    if not done:\n",
        "  # But if it is not in the terminal state, and there are few more\n",
        "  # Actions to be played. We are going to calculate the total and\n",
        "  # Leave all as the current reward.  so in this if statemant, the \n",
        "  # reward is to reward plus self dot gamma multiplied by np.dot\n",
        "  # Amax. These function returns are the maximum value from an input\n",
        "  # Array and that is exactly what we want. We want to return the highest\n",
        "  # Value of predictions.\n",
        "  # Inside that function we provide self dot model dot predict \n",
        "  # of next_state and said that to zero[0] beacuse of the output size\n",
        "\n",
        "      reward = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
        "  # And that is our discount that totals the reward after \n",
        "  # They have defined the target variable which is predicted by \n",
        "  # The model as well.\n",
        "  # So target is equal to self dot model dot predict of state.\n",
        "  # At this point, It is just an action and we want to modify\n",
        "  # It with our current reward and this is exactly why we use a means \n",
        "  # Squared error instead of crossentropy loss \n",
        "    target = self.model.predict(state)\n",
        "  \n",
        "  # the target Of zero because of the output shape and the action\n",
        "  # And this is the action performed and it is all equal to reward\n",
        "    target[0][action] = reward\n",
        "  \n",
        "\n",
        "  # Now that we have our target and our state, we can then feed the\n",
        "  # Model in just writing self dot model dot fit.\n",
        "  # And always provide state for the features and target for our target\n",
        "  # Epochs just set to 1 because we train the model very often on each\n",
        "  # Sample from our batch. We do not want to rpint all these  results\n",
        "  # Just put verbose is equal to zero at the end of this function.\n",
        "    self.model.fit(state, target, epochs=1, verbose=0)\n",
        "\n",
        "  # Lest's decrease the epsilon parameter se we can stop performing\n",
        "  # Random actions at one point.\n",
        "  # If self dot epsilon is bigger that self dot epsilon final. \n",
        "  if self.epsilon > self.epsilon_final:\n",
        "  \n",
        "  # And if that is correct, let's decrease it by multiplying it with\n",
        "  # Epsilon decay\n",
        "    self.epsilon *= self.epsilon_decay\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFj8wA2yWbzQ",
        "colab_type": "text"
      },
      "source": [
        "**Stage-4: Dataset Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c36r9C7WxF2",
        "colab_type": "text"
      },
      "source": [
        "Defining helper function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQN1c9mtXZZt",
        "colab_type": "text"
      },
      "source": [
        "**Sigmoid**\n",
        "\n",
        "The sigmoid is an activation function used mostly at the end of the network.\n",
        "\n",
        "When we have binary classification ultimately it scale a number to the range from 0 to 1. This function here help us to scale our price.\n",
        "\n",
        "\n",
        "We are doing this so we can compare and gather the real difference between each day.  For one day the stock can be 200 and jump the next day to 1000.\n",
        "\n",
        "\n",
        "\n",
        "The difference between them is the same as forty five(45) to 200. The difference in the jump is the same but the price cannot handle this.\n",
        "\n",
        "So we need to use sigmoid function to have the same number to represent this difference.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxcLmDXlWwgr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(x):\n",
        "  return 1/ (1 + math.exp(-x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXGgXEX_X4SQ",
        "colab_type": "text"
      },
      "source": [
        "**price format function**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHkstMx4USb5",
        "colab_type": "text"
      },
      "source": [
        "This function helps hus to print out the price of the stock we bought or sold. \n",
        "\n",
        "It says if the number is negative. So we lost some money for example it will add a minus in front of it and we are using this string formating.\n",
        "\n",
        "\n",
        "To limit the printed number to only 2 decimal points.\n",
        "\n",
        "We are doing the same thing fi the number is positive although we don't add the minus in front of it.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ce8eoxxAz_7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stock_price_format(n):\n",
        "  if n < 0:\n",
        "    return \"- $ {0:2f}\".format(abs(n))\n",
        "  else:\n",
        "    return \"$ {0:2f}\".format(abs(n))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uUuVqEiabZB",
        "colab_type": "text"
      },
      "source": [
        "**Dataset loader**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSQKlrgIVfTp",
        "colab_type": "text"
      },
      "source": [
        "Let's define a dataset variable and call from our library the data_reader. \n",
        "\n",
        "This function has a data either object that takes a stock price information. \n",
        "\n",
        "\n",
        "As we can see, it can reach to Google finance, Yahoo finance or any other provider of stock market information.\n",
        "\n",
        "\n",
        "In our case, we are going to use Yahoo finance. It provide the mos relevant information for us. \n",
        "\n",
        "Ths stock name represent the name of the company on the stock exchange market.\n",
        "\n",
        "Let's use Apple for this example to get Apple stock information, we write a sting \"APPPL\"\n",
        "\n",
        "\n",
        "The next argument that we have to specify is data source or which provider we should ask to provide information for us.\n",
        "\n",
        "\n",
        "In our case, we saying data source is equal to Yahoo\n",
        "\n",
        "We are going to copy and paste right here in the function but before we do that let's check what it does.\n",
        "\n",
        "It will check through Yahoo\n",
        "Get Apple stock information and save that to our data set variable\n",
        "\n",
        "It uses pandas 's data frame object to store that information\n",
        "\n",
        "Now we dont' have to use all these information if we want to pedict stock market price.\n",
        "\n",
        "Here we are going to use only the close column for our example.\n",
        "\n",
        "That is the coulumn we are trying to predict and also from these data we are going to build state for our network.\n",
        "\n",
        "This index here is the date in here we need to specify the starting date and the end date \n",
        "\n",
        "\n",
        "And if we specify dataset dot  index, it is going to take all the data from all the data set.\n",
        "\n",
        "But if we specify just the first one it is going to get our starting date and that's pretty much it.\n",
        "\n",
        "We can convert this date to a string format and split it on  space so we don't use this time or related information by just the date\n",
        "\n",
        "So this is the same thing that is happening in our function\n",
        "\n",
        "We are taking the first and the last date from our dataset putting them in the string format and getting the information like what what is the starting and the ending date of the dataset\n",
        "\n",
        "\n",
        "Instead of specifying Apple as a string let's use the argument of the function instead ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZ8qIyzXAz85",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dataset_loader(stock_name):\n",
        "  # Complete the dataset loader function\n",
        "  dataset = data_reader.DataReader(stock_name, data_source=\"yahoo\")\n",
        "\n",
        "  start_date = str(dataset.index[0]).split()[0]\n",
        "  end_date = str(dataset.index[-1]).split()[0]\n",
        "\n",
        "  close = dataset[\"Close\"]\n",
        "  return close"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_pk_oQMcOI9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = data_reader.DataReader(stock_name, data_source=\"yahoo\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noc-L9rwWGpU",
        "colab_type": "text"
      },
      "source": [
        "**State Creator**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsGyhpOBD0V1",
        "colab_type": "text"
      },
      "source": [
        "This function takes data and generates states from it.\n",
        "\n",
        "first let's see how to transform the problem of stock market trading to a reinforcement setting on this graph.\n",
        "\n",
        "We have information about Aplle stock price from 2010 until today. \n",
        "\n",
        "On the x axis we have time information \n",
        "And on the y axis we have prices of stocks on a specific day which is not visible.\n",
        "\n",
        "It is taken from Yahoo Finance as we can see here.\n",
        "\n",
        "These blue main line on the graph shows how the value of a company changes over time.\n",
        "\n",
        "Let's see how to modify these data so that a reinforcement algorithim can understand it.\n",
        "\n",
        "Each point ont this graph is nothing more but the floating ni=umber that represents the price stock today.\n",
        "\n",
        "Our task is to predict what is going to happen next; Is the price going up or going down and the next day based on that information, our agent will determine what to do: sell, buy, or do nothing.\n",
        "\n",
        "Let's take six data points here or six days for example and transform them or this part of the graph into numbers. The red part right here si what we are trying to predict and that is our target.\n",
        "\n",
        "\n",
        "Well Let's take this red line and transform it into number and let's say it is forty seven points six\n",
        "47.6. These numbers are totally random\n",
        "\n",
        "They are not taken from the graph\n",
        "\n",
        "In this example here we have the window_size is equal to 5 which is also the argument of the state create function.\n",
        "\n",
        "Based on this argument we determine how many previous days we consider before predicting the current one.\n",
        "\n",
        "Now we have our state made of 5 numbers where each number represents one day in the past.\n",
        "\n",
        "Based on this it doesn't look right this is nothing  more but a regression problem.\n",
        "\n",
        "We have some numbers and we are predicting our target which is also continuous.\n",
        "\n",
        "Well let's modify these solutions so we have our actions instead of the real number of targets for our estate part\n",
        "\n",
        "\n",
        "We can still use a row numbers but that won't help us anymore\n",
        "\n",
        "Since our target is not the real number or real price but an action, lets change our state to use the difference between days as our state. \n",
        "\n",
        "These information will represent price shnages over time and potentially catch the trend  in the future as well.\n",
        "\n",
        "\n",
        "Now ae have a state and this new state has only 4 numbers when our window size had 5. We will handle this in code.\n",
        "\n",
        "This was just for a demonstration. Because we have changed information over time, our action and the new state is bought because prive was prtty high and we expect to drop again.\n",
        "\n",
        "So nased on that information we are going to performbuy stock at the new state.\n",
        "\n",
        "Let's go back and implement this in the code.\n",
        "\n",
        "let's implement this strategy. Here we have a function called state creator that takes three arguments data which is:\n",
        "- the stock market: Data downloaded with the dataset loaded function\n",
        "\n",
        "- Timestamp: This is the day in the dataset that we want to predict for.\n",
        "Il could be anywhere from 0 to the landfall data.\n",
        "\n",
        "- And lastly, we have window size: This argument determines how many previous days we want to consider in our state to predict the current one\n",
        "\n",
        "This argument goes anywhere from one to the line of data.\n",
        "\n",
        "We can play with this window size parameter and see what is the best size for the company that we are trying tp trade for.\n",
        "\n",
        "In this section, let's work with window size of 10.\n",
        "\n",
        "The first thing to do is calculate the staring ID so let's write ID is equal to timestep minus window_size plus 1\n",
        "\n",
        "Now we have to calculate the new starting date of our state.\n",
        "\n",
        "It is calculatedlike this timestep minus window size plus 1\n",
        "\n",
        "\n",
        "for example when the timestep is zero or our agent is just starting  and the window size si 10, the starting ID is minus 9.\n",
        "\n",
        "The plus 1 is added because the way we create our state.\n",
        "\n",
        "We don't want prices on certain days but as we explained differences between the current price and the  previous price, to see that change between dates, we start with plus 1.\n",
        "\n",
        "\n",
        "Now that we have know our stating ID, we need to handle two diferent cases when the starting ID is negative and when it is positive.\n",
        "\n",
        "If the starting ID is bigger than or equal to zero, this will handle the case when the starting ID is positive\n",
        "\n",
        "When we have that situation, we create state like this. We know data is equal to data of starting a day until timestep plus 1.\n",
        "\n",
        "If that is not the case and our starting ID in negative, we will append the first day info as many times as we need to match with the window size of a data window size over data\n",
        "\n",
        "\n",
        "Window data is equal to then  put minus in front of starting ID because starting ID at this point is negative then multiplied by the list of data of zero.\n",
        "\n",
        "This will replicate this member many times and now we need to spend the rest of the element to have the full window size of data.\n",
        "\n",
        "\n",
        "Plus list of data from zero 2 timestep plus 1\n",
        "\n",
        "Now we have our data from which we can create our state of data.\n",
        "\n",
        "Let's define an empty list called state is equal to empty list.\n",
        "\n",
        "And after that we can iterate through te whole window data list for i in range window size minus 1\n",
        "\n",
        "The minus 1 is here because we have differences between the current element and the one after.\n",
        "\n",
        "State append.\n",
        "\n",
        "We have now to normalize the difference between the next day and the current day because prices can be very different.\n",
        "\n",
        "We want to scale the difference between prices on the same scale so we have the same difference no matter the price.\n",
        "\n",
        "We are going to do that with sigmoid function.\n",
        "\n",
        "So let write sigmoid of window data of i plus 1 minus window data of i\n",
        "\n",
        "Return  numpy array of the state and we are done.\n",
        "\n",
        "Now we completed the function that is going to create the state for us\n",
        "\n",
        "The same state and the same methode that was explained on the graph.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nv95wkE4Az5p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def state_creator(data, timestep, window_size):\n",
        "  starting_id = timestep - window_size + 1\n",
        "  if starting_id> 0:\n",
        "    windowed_data = data[starting_id:timestep + 1]\n",
        "  else:\n",
        "    windowed_data = -starting_id * [data[0]] + list(data[0:timestep+1])\n",
        "  \n",
        "  state = []\n",
        "  for i in range(window_size - 1):\n",
        "    state.append(sigmoid(windowed_data[i+1] - windowed_data[i]))\n",
        "\n",
        "  return np.array([state])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgMreTEJcKMN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMi2K8RtYBbq",
        "colab_type": "text"
      },
      "source": [
        "**Loading a dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wppq3TursfZ3",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUZFv85jAz2Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stock_name = \"AAPL\"\n",
        "data = dataset_loader(stock_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Guz5BVykYXEU",
        "colab_type": "text"
      },
      "source": [
        "# Stage-5: Training the AI trader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkXTIn6_YikR",
        "colab_type": "text"
      },
      "source": [
        "### Setting hyper parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pu2UAskoxA7t",
        "colab_type": "text"
      },
      "source": [
        "The first hyper parameter is the window size. It is equal to 10. That means that we are going to use 10 previous days to predict the current one in the previous section.\n",
        "\n",
        "\n",
        "We used word epochs before but in the the reinforcement learning we use rather episode.\n",
        "\n",
        "So we need to define how many times we are going to run the whole dataset or the whole environment.\n",
        "\n",
        "in our case here we will say episode is equal to 1000. \n",
        "\n",
        "The algorithm is going to run very slowly.\n",
        "\n",
        "So we want to wait for all of them to pass then we  will specify 32 for the battch size.\n",
        "\n",
        "And at the end, data sample is going to be equal to len of data minus 1\n",
        "\n",
        "Since we are trying to predict the next day we can't use the last one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQqP0C6_96xu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "window_size = 10 \n",
        "episodes = 100\n",
        "batch_size = 32\n",
        "data_samples = len(data) - 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLM7c2T3Y8Vk",
        "colab_type": "text"
      },
      "source": [
        "### Defining the Trade Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgQB_ctTzqhg",
        "colab_type": "text"
      },
      "source": [
        "Now it is time to define our tarder bot.\n",
        "\n",
        "Let's call it trader and it is going to be equal to our class AI trader. We have to remember that it takes a lot of arguments.\n",
        "\n",
        "But since we defined the action space to be free as default and the model name to be a trader by default.\n",
        "We wont' change that. \n",
        "\n",
        "\n",
        "So the only thing that we have to specify is the state's size.\n",
        "\n",
        "For us that is our window size \n",
        "\n",
        "Let's track the model structure.\n",
        "\n",
        "Trader dot model and there is always dot summary\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csKC-vy496uH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trader = AI_Trader(window_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hE4LtVr096p_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trader.model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjPrB4RedSuC",
        "colab_type": "text"
      },
      "source": [
        "### Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KOZugxhuu08",
        "colab_type": "text"
      },
      "source": [
        "**Definition of the training loop**\n",
        "\n",
        "As always, it is going to be a for loop to iterate through all episodes that we define.\n",
        "\n",
        "And we defined 1000 episodes.\n",
        "For episodes in the range of 1, episodes plus 1.\n",
        "\n",
        "The next thing to do is to print out the current episode so we can kepp track of the training process.\n",
        "\n",
        "So let's print episode and use these kind of formating. We can use this format function but we need just to specify 2 things since we have 2 curly brackets that we need to populate. \n",
        "\n",
        "So we will specify episode and episodes. We are just counting how many episodes are left.\n",
        "\n",
        "\n",
        "Let us define our initial state and the initail state is alwyas the same.\n",
        "\n",
        "\n",
        "At the very beginning of the episode, we define it as a state and it is equal to creator which takes 3 arguments: data, 0 for at this point, our timestep is zero. And lastly we have window_size plus 1.\n",
        "\n",
        "Then we are going to define 2 variables so we can keep track of that.\n",
        "\n",
        "The first one is total_profit. Actually, we don't have to specify this, but if we want to see how the model is progressing over time, it is recommended to have this variable.\n",
        "\n",
        "\n",
        "The second one is trader dot inventory so we can access our inventory. It is an empty list.\n",
        "\n",
        "We need to remember that our inventory is just a python list that stores all stock that we bought.\n",
        "\n",
        "But sometimes, we can finish our episode without selling all the stocks. \n",
        "\n",
        "So we want to start our episode clean without any stock in the inventory.\n",
        "\n",
        "So here at this point we are just making sure that we have clean inventory before we start our episode.\n",
        "\n",
        "\n",
        "After, that let's define our timestep. It represents how many samples we have. One timestep is one day.\n",
        "That is why timesteps repersent how many samples we have.\n",
        "\n",
        "Thus we write for t in tqdm(tqdm is just used to visualize the progress bar) in range. We have always to provide data sample that we defined above as our hyper parameter.\n",
        "\n",
        "The first thing that we have to access is an action. That action is going to be taken by the model.\n",
        "\n",
        "\n",
        "So we say action is equal to trader dot trade and here we nned to provide our state.\n",
        "\n",
        "Thes actions are going to totally randomly selected after some tome when the model is trained enough. It is going to take these actions by itself.\n",
        "\n",
        "\n",
        "So Now that we have an action, it is time to perform it to get to the next state.\n",
        "\n",
        "Let's define next state is equal to state_creator which take data as always then t plus 1 sine we want the next state and not the current one and again window size plus 1.\n",
        "\n",
        "Now define a reward because we didn't calculate anything at this point. The reward is going to be zero\n",
        "\n",
        "\n",
        "So action 1 is buying \n",
        "Action 2 is selling\n",
        "\n",
        "We can only trade with actions or stocks that we already bought.\n",
        "\n",
        "The next ting to do is to define an if statement which checks if an action performed right now by the model is 1 or is we bought a stock.\n",
        "\n",
        "We are going to say if an action is equal to 1 and if that is the case, the agent is buying. \n",
        "\n",
        "In that case the only thing that we have to do is to append the current stock to the inventory.\n",
        "\n",
        "To do that we are going to define trader dot inventory sinec it is on the python list and call append of data t because we wnat the current dat iinformation to be our bought stock. \n",
        "\n",
        "We can ad here something like agen bought the stock for x price.\n",
        "\n",
        "\n",
        "Ok We checked if the action was 1 meaning if the agent was buying stocks rather selling them.\n",
        "\n",
        "Now we are goint to check if the agent is selling stock meaning id action is equal to 2.\n",
        "\n",
        "We know that we cannot sell stocks if we haven't previously bought them.\n",
        "Again we cannot sell stocks if they are not in our inventory.\n",
        "\n",
        "Thus we nned to make sure that we already have something in our inventory.\n",
        "\n",
        "To handle this situation, we are going to introduce an additional condition.\n",
        "\n",
        "Hence we say if len of our trader dot inventory is bigger than 0 and in other words, we have some stocks bought already.\n",
        "\n",
        "So it both conditions are true, we are going to track what is the buy price.\n",
        "\n",
        "Let's say buy_price is equal to and we will use our trader dot inventory to pop of zero.\n",
        "\n",
        "By doing this, we are selling stock in ; and that is an additional strategy that we can use to improve this algorithm.\n",
        "\n",
        "\n",
        "Now we are going to calculate the reward by using max between date of t which is the current dat of our stock minus buy_price and 0.\n",
        "\n",
        "This means that if dataof t is less than buy_price, we lost money and the reward here is zero.\n",
        "\n",
        "\n",
        "The toptal_profit is increased right here with the difference between te current price and the buy_price.\n",
        "\n",
        "The whole user experinece can be improved by using simple print statement here such as the agent has bought or sold the stock for that price  and the week it earned or lost taht amount of money\n",
        "\n",
        "The nice thing to do is to check whether or not this is the last sample in our dataset and if that is the case, we are done  for we do not have  any more steps to perform in the current episode so we can say if t is equal to data_sample minus 1 if that is the case , we say done is equal to true ortherwise we say done is equal to false and that is pretty much it.\n",
        "\n",
        "\n",
        "The next thing to do is to append all the data to our trader memory or experience replay buffer\n",
        "\n",
        "To do this, we say trader dot memory dot append, we call this function since we are using just a simple python list and now we have to provide a lot of things;\n",
        "\n",
        "Let's say state, action, reward, and then we have to specify next state we calculated and lastly we add done.\n",
        "\n",
        "This is all what we have to provide to our memory then we are going to shange the state to our next state so we can iterate through the whole thing.\n",
        "\n",
        "The next step is to print out the total profit.\n",
        "\n",
        "Before we start with our training process, there are two more things to do and that is to check if we have more information in our memory of our batch_size.\n",
        "\n",
        "We are going to call len of trade dot memory is bigger than batch_size\n",
        "If that is the case, we are going to call trade dot batch_train and the only argument that we need to provide is batch_size.\n",
        "\n",
        "Now in he main episode loop we are going to check if the number of episodes in a total division of 10 is equal to zero.\n",
        "\n",
        "And if that is the case, we are going to save the model\n",
        "\n",
        "\n",
        "To save the model, let's specify trader dot model.save and inside this saved function, the only argument to provide is the model name.\n",
        "Let's use the same name of our class: ai trader then the curly brackets\n",
        "\n",
        "So we can populate with episode index then dot h5 which is the extension of our weights.\n",
        "And since we need to populate these curly barckets dot format and in that provide episode.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OChvJHpw96mr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for episode in range(1, episodes + 1):\n",
        "  print(\"Episode: {}/{}\".format(episode, episodes))\n",
        "\n",
        "  state = state_creator(data, 0 window_size + 1)\n",
        "  total_profit = 0\n",
        "  trader.inventory = []\n",
        "\n",
        "  for t in tqdm(range(data_samples)):\n",
        "    action = trader.trade(state)\n",
        "\n",
        "    next_sate = state_cerator(data, t+1, window_size + 1)\n",
        "    reward = 0\n",
        "\n",
        "    if action == 1:\n",
        "      trader.inventory.append(data[t])\n",
        "      print(\"AI Trader bought: \", stock_price_format(data[t])) \n",
        "    \n",
        "    elif action == 2 and len(trader.inventory) > 0:\n",
        "      buy_price = trader.inventory.pop(0)\n",
        "\n",
        "      reward = max(data[t] - buy_price, 0)\n",
        "      total_profit = total_profit + data[t] - buy_price, \"Profit\"\n",
        "\n",
        "    if t == data_samples - 1:\n",
        "      done = True\n",
        "    else:\n",
        "      donn = False\n",
        "\n",
        "    trader.memory.append(state, action, reward, next_state, done)\n",
        "    state = next_state\n",
        "\n",
        "    if done:\n",
        "      print(\"##############################\")\n",
        "      print(\"TOTAL PROFIT: {}\".format(total_profit))\n",
        "      print(\"##############################\")\n",
        "\n",
        "    if len(trader.memory) > batch_size:\n",
        "      trader.batch_train(batch_size)\n",
        "\n",
        "    if episode % 10 == 0:\n",
        "      trader.model.save(\"ai_reader_{}.h5\".format(episode))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kMAVKts96jb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOr_mrD796gg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHS_OOuP96c5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoNyG6Mz96Zp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPzylrrc96Vo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQA96wR-96R_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGTqOrpH96Oc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfG4Jk6496Hq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEQHWYta96DT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}